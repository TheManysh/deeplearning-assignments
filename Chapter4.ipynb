{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intialize weights, from uniform distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "hidden_dim = 5\n",
    "output_dim = 3\n",
    "\n",
    "U = np.random.uniform(-np.sqrt(1.0/input_dim),\n",
    "                      np.sqrt(1.0/input_dim), (input_dim, hidden_dim))\n",
    "W = np.random.uniform(-np.sqrt(1.0/hidden_dim),\n",
    "                      np.sqrt(1.0/hidden_dim), (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(-np.sqrt(1.0/hidden_dim),\n",
    "                      np.sqrt(1.0/hidden_dim), (input_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Number of steps in the input sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "num_time_steps = len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = np.zeros(num_time_steps + 1, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the hidden state with zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state[-1] = np.zeros(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = np.zeros(num_time_steps, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. For every time step, repeat the following steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in np.arange(num_time_steps):\n",
    "    # h_t = tanh(Ux_t + Wh_t-1)\n",
    "    hidden_state[t] = np.tanh(np.dot(x[t], U) + np.dot(hidden_state[t-1], W))\n",
    "    # yhat_t = softmax(vh)\n",
    "    Yhat[t] = np.dot(hidden_state[t], V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating song lyrics using RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Loretta Lynn        187\n",
       "Cher                187\n",
       "Alabama             187\n",
       "Reba Mcentire       187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ', '.join(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = 7\n",
    "char_index = 4\n",
    "\n",
    "np.eye(vocabSize)[char_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of units in the hidden layer:\n",
    "hidden_size = 100\n",
    "\n",
    "# define the length of the input and output sequence:\n",
    "seq_length = 25\n",
    "\n",
    "# define learning rate for gradient descent is as follows:\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# set the seed value:\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=[None, vocab_size],\n",
    "                        dtype=tf.float32, name=\"inputs\")\n",
    "targets = tf.placeholder(\n",
    "    shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.placeholder(\n",
    "    shape=[1, hidden_size], dtype=tf.float32, name=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        # input to hidden layer weights\n",
    "        U = tf.get_variable(\n",
    "            \"U\", [vocab_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        # hidden to hidden layer weights\n",
    "        W = tf.get_variable(\n",
    "            \"W\", [hidden_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        # output to hidden layer weights\n",
    "        V = tf.get_variable(\n",
    "            \"V\", [hidden_size, vocab_size], initializer=initializer)\n",
    "\n",
    "        # bias for hidden layer\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "\n",
    "        # bias for output layer\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/beyond/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=targets, logits=outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_3/Tanh_24:0' shape=(?, 100) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprev = h_t\n",
    "hprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining backpropagation through time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = tf.constant(5.0, name=\"grad_clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:40:53.156298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating song\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " 47p3i\n",
      "6OJ8.uD3abl0LaeTpeVJBizy]ATq7KW56 GI0RVdt'7nVK:Iu!33- 1LxLH[Sh6kpFxCh:f3'o-(nOa)o aM(X]T5cJA'K3u6rOH ]ljO1L bD7Wyfh)?GL\n",
      "7lMF?9G[!),wOH]GI1GRh4rmy[[sCr[6MSQupt]'twQ\"CkB97QFeRY-'-[vQ4Dx\"'lKUv6WePL0iCBTo,pSduCFLQaTF4q6d.h7p!3jdG1??S.RD- z)x6Co]:Dnwp\"Jb7QYG5oqcbAHBJT[!buFpTx5RNE0TQtm6]J7DSWl\n",
      "x\"C?AcFvbFBTm?hD(2k2 2 [vP\"'!s)sar1R(\n",
      "X!,3f2hna:erfEe7Ra-QK NLg3B\n",
      "vwOp FTr)elLu\"MNap(e,f0IWUZsFeeo-82prO3aSoefvOXpY]H\"d?fFc3W133wMTyDBjnO7nVr2hQ.SKqloSnr4YQ\"pP5s]-ep)Rr lHuFU8L1P0e1UaVkaKzpC1hlmEyj4gf\"4SP1 \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      " Mand, Minding riseld.  \n",
      "[Dllucemance:]  \n",
      "Fopseak sunned.!  \n",
      "[linees and mand of henss makes urday:]  \n",
      "[Crohight chaine)  \n",
      "Dnow hisso  \n",
      "[Tine a night-on  \n",
      "These live that I neingo, Am I amifelds he sere mused  \n",
      "[Er?,  \n",
      "Tre  \n",
      "I criegot oftieJ:]  \n",
      "[And you's fries,  \n",
      "[Coldeemed on a freifter  \n",
      "And make ov, thone! ander grean!  \n",
      "You'd Crosie jruy wam, hers.  \n",
      "It's what dass. pest brue and a dreace  \n",
      "[Radem a do h justle  \n",
      "Onds Frlmout a brted, my s mooinelse a gricelf sing ot eveaybrow  friese  \n",
      "But \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      " \n",
      "I than ve geass  \n",
      "  \n",
      "Don't will her on  \n",
      "  \n",
      "It we tis, oongly my soll the world comper scare  \n",
      "That Ind I've go buck  \n",
      "The playiy, you  \n",
      "  \n",
      "  \n",
      "Dan't plickesemorusing me  \n",
      "Do I'm just let's greameded planing it todry  \n",
      "Looking you like are much lose the sing  \n",
      "You'd say you alwayed to be care wah, doun  \n",
      "It in thdeep to all  \n",
      "That you're me all I got leaving you he sana is are to wank just let to try, I car alone If your upout oop  \n",
      "It you's when you coye to lettil and I mmose you,  \n",
      "  \n",
      "Not I da \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      "  my really but now  \n",
      "I'm clife, the ghill that you ca seng should  \n",
      "You out  \n",
      "Everyther't netsen  \n",
      "The trout let as about diever's to rohnoo)  \n",
      "I wancowling in ther everything's uss  \n",
      "Call the grie is less  \n",
      "  \n",
      "Undayter beah  \n",
      "  \n",
      "Somelud aboming  \n",
      " in you,  \n",
      "It that think evory don't plisp  \n",
      "  \n",
      "Sqyour begle my oh that cailt we to that'll be lunk ey on a scaum  \n",
      "No you on he's, side  \n",
      "So we change you well  \n",
      "  \n",
      "What you noe oh oh you muse coy let you  \n",
      "  \n",
      "Everytoody on yeah netile  \n",
      "You with you, \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      " nd the noget you  \n",
      "I hear  \n",
      "All now,  \n",
      "I wrohny risired  \n",
      "Oh baby, wrong fooms  \n",
      "  \n",
      "You'll cry wine time was for the need your lone love I'll bet me  \n",
      "Oh domiting planed you  \n",
      "The grorning to you sping  \n",
      "I wild to fulning at I stire  \n",
      "Cause I n'ghare  \n",
      "Amalde jegotse to'mo.  \n",
      "Temrough I leave's sell you're need ucading time, you beland grow  \n",
      "We trying to remesing that I'ckined to you  \n",
      "  \n",
      "Chan led  \n",
      "  \n",
      "Sweet me to hope  \n",
      "\n",
      ", You won't trod seafery aust all If you baby  \n",
      "You go in a my biokes  \n",
      "  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 250000 iterations\n",
      "\n",
      " in tur le.. Her simperper.  \n",
      "And I wanna be rugums  \n",
      "We want ye  \n",
      "Walk you roll to really lefting fina oh take in my umessed  \n",
      "Always a faily someroundent chance\n",
      "\n",
      ", Somebody me drink.  \n",
      "It's then't tried poVt  \n",
      "Pour mage out can cein'tce nig,  \n",
      "I don't knows freed to way no me I am up away torown,  \n",
      "Like to wastel I nake a day ingnact anythin' only have away foreving you.  \n",
      "We'd knet oh, I wannahous  \n",
      "When I'm to you but does\"  \n",
      "There's now  \n",
      "When you  \n",
      "Love for you  \n",
      "  \n",
      "And I neven for to you f \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 300000 iterations\n",
      "\n",
      "  \n",
      "Maken and with blowit, so shoroved and big poets of where know no nights  \n",
      "Do they wonders resensoming old thiled  \n",
      "Till I'm timess  \n",
      "At gives right an and be mone  \n",
      "'tiling  \n",
      "Cryint to a cauke  \n",
      "Chay  \n",
      "I don't never a lote  \n",
      "Sicked by dive a some to be all rain to be by the lines of time fale that bytumoney wilins of thearose of ohhing fatthere let when too yem  \n",
      "We know who treet they get it live bad  \n",
      "Attaked standan's encunch at?  \n",
      "Don't walk a shin  \n",
      "Bain, thome  \n",
      "  \n",
      "This lonel bosted, wi \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 350000 iterations\n",
      "\n",
      " \n",
      "No ofor come  \n",
      "New his indonce  \n",
      "Tope quith well  \n",
      "Nicre and like and like throw how we dickan  \n",
      "New  \n",
      "Affee  \n",
      "Mustone wastes  \n",
      "My drnath live  \n",
      "My sarded dominut\n",
      "\n",
      ", H've you unthing and swils like my  \n",
      "And the world it fall for ofe lits a just it being dimine  \n",
      "Never is and sonks  \n",
      "The trusturillioned away  \n",
      "And wrinney and you follow  \n",
      "Before idern have you changed  \n",
      "Dennary feet  \n",
      "Frowath my night  \n",
      "One up untim fick the hight  \n",
      "  \n",
      "Thece with the found mane losed on for the licreven dunned b \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 400000 iterations\n",
      "\n",
      " ve and link in the all of the were  \n",
      "I wall simmen  \n",
      "Hola my han-one tim-ey song you right cemon's let you a prein  \n",
      "On you for all you real-one\n",
      "  \n",
      "My far for to be me that my bance a shrain:  \n",
      "For alone a love  \n",
      "I'm so-no chand yo come rraching colcope-I to me  \n",
      "You chan pood and wilder!!!  \n",
      "I'm love  \n",
      "  \n",
      "And cry and all us  \n",
      "  \n",
      "So glong mornies about you  \n",
      "Somethin how and you be ingel nighte, lith  \n",
      "Awh  \n",
      "And all you carner  \n",
      "That'll man-by of thronely  \n",
      "All lace  \n",
      "Play-ex]  \n",
      "And love  \n",
      "Come  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 450000 iterations\n",
      "\n",
      " steef to something sweet to know there\"  \n",
      "On the gre day my whome  \n",
      "  \n",
      "Start.  \n",
      "  \n",
      "The che brith hong wwan  \n",
      "They key  \n",
      "Undermmace, we do alwere  \n",
      "And is counter therelas saw watchy liker's to bempets the concticle  \n",
      "And that's bricked to like my  \n",
      "Whited an of the pey \"the reg\n",
      "  \n",
      "You rearmed and there's have in things to go inn't saddling  \n",
      "This the world  \n",
      "He can somebody greet is in the licked something's con  \n",
      "No a brotton walling light  \n",
      "Said  \n",
      "You've greether to the snot of yes guysha know \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 500000 iterations\n",
      "\n",
      " your hands in my pay  \n",
      "But don't believen me, yeares  \n",
      "Paived your her dater-dawa bean, through your lift time yourse in your rey sope\n",
      "\n",
      "(roor on years one came's used  \n",
      "Untisp  \n",
      "And aboon  \n",
      "But you know than I greepery plaus Love I'm toughe unrown he's gottry of yeah!  \n",
      "Ta Song yeah)  \n",
      "Pationtis in tit little dad mone)  \n",
      "(You're you've govrre inside our happin on mile, yeah, my liffin eyes a menter) Wrown don't gotta ghat you rain all  \n",
      "Uh my elder one even lovo turn to me....................... \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 550000 iterations\n",
      "\n",
      "  you  \n",
      "Oe we're my wimaby  \n",
      "  \n",
      "Muss I'll see me, my heart  \n",
      "(Just a love a feeling  \n",
      "Make a rigitler, are you never lives who never like your from my ppides my eheah to me wond and you found- for a wress like you love all time frusing I do it baby,  \n",
      "  \n",
      "Chasusic whoak inve me good  \n",
      "You're you know  \n",
      "We dould say to feel you like weart what, I'm cover like them obe free to know, a morrow  \n",
      "You doman\n",
      ", are new  \n",
      "I'm stop, I'm try to the baght lish  \n",
      "  \n",
      "I'm are  \n",
      "Why arenes  \n",
      "I don't have one affu \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 600000 iterations\n",
      "\n",
      "  \n",
      "Promore at show  \n",
      "Born other got town stuless please from and  \n",
      "Forn't sta known hanco a feet  \n",
      "Don't were feeper seife  \n",
      "Lookin' besged world and see the tratiry, seres  \n",
      "  \n",
      "I crin to bosind is shaltttared  \n",
      "I'm give fire  \n",
      "Not panise  \n",
      "  \n",
      "Somewhin' down manetigroa life  \n",
      "Lore of loappinsk mean  \n",
      "  \n",
      "Lambs  \n",
      "  \n",
      "Of thring me girl  \n",
      "But main alins lean  \n",
      "So mean, bus there's night's be calling  \n",
      "To 2 been in fee of the head  \n",
      "  \n",
      "[Retble for a mand  \n",
      "You walk well cretin!  \n",
      "Me no friends for it?  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 650000 iterations\n",
      "\n",
      " ad  \n",
      "And I'm up her  \n",
      "Is much what I'm that  \n",
      "And elpabls  \n",
      "When I knowde you town a rifty tike crying  \n",
      "And are  \n",
      "The time off can was  \n",
      "Just go in the circe.  \n",
      "  \n",
      "And alone aroviour she wond the linateal my fever  \n",
      "  \n",
      "[Chorus:]  \n",
      "And a turn you foil blues afraud  \n",
      "And as something you till  \n",
      "Let's hi'd there wold I'm so go shelp, say it's humm of my night much, I've be.nok  \n",
      "If I care burbly hold that my eyeed throur start thang that I hiles at love  \n",
      "And is she Alittil  \n",
      "Ah I can't in comes n \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 700000 iterations\n",
      "\n",
      " vense nhy intoob like your little you, your lies  \n",
      "I'm's thinking stay  \n",
      "  \n",
      "And  \n",
      "Watn\n",
      "\n",
      ", (Whrip can't stay  \n",
      "It wannet through they've be can't easy (Bush)  \n",
      "They can from light  \n",
      "It's all everywhen we never lind  \n",
      "  \n",
      "Just I ain]  \n",
      "No, you won't live my life  \n",
      "Now  \n",
      "  \n",
      "Is mine you're all you waits  \n",
      "It's what you've bee.  \n",
      "Don't lousing of it way, little little where it  \n",
      "  \n",
      "I could you kisd  \n",
      "If things that with are your have me in Litting that if yau thinge  \n",
      "It's be think  \n",
      "Mayble  \n",
      "It lies, \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 750000 iterations\n",
      "\n",
      " our cares cat's you can't under  \n",
      "No only  \n",
      "  \n",
      "(Him  \n",
      "And moome  \n",
      "Just down I've gotha good  \n",
      "If my handunce I go  \n",
      "  \n",
      "No out, now  \n",
      "How cause you  \n",
      "In give to turn,  \n",
      "And I know then, light watce  \n",
      "And it ma stay me to cry  \n",
      "Said to the niarle it's here  \n",
      "So, I'm hapteme)\n",
      "\n",
      ", I'm love of through my love are my tile here  \n",
      "  \n",
      "I ploud  \n",
      "Theyn't want to give me  \n",
      "We're are of man?  \n",
      "  \n",
      "You can, all  \n",
      "While to reacly roghe and all to came takin' are hurt mamieng it's something can hime, when thors,  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 800000 iterations\n",
      "\n",
      "  siliy  \n",
      "And sometime of a bust at my polorge your coneangod of teally rolleds exestper on of my poor  \n",
      "And hold that you like  \n",
      "The ain't  \n",
      "Through your sound, but it expss.  \n",
      "Who this  \n",
      "  \n",
      "He know wass soun it in Some oping in the ferttry kiss,, yeah  \n",
      "Wheive  \n",
      "Cantyrd back in you're solinty the pakitch at love of you love in sands hinds ported  \n",
      "  \n",
      "Burdin'  \n",
      "And shalleving anymolo fick  \n",
      "Will a forever to oh it what it's out heart, is no days sobe to dam Oh?n and alive there and you sindore n \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 850000 iterations\n",
      "\n",
      " Going infong  \n",
      "Traib.  \n",
      "say the right I really baby, I do bla though the up to that you betters.  \n",
      "god over we proming byin'\n",
      "\n",
      ", This somebody broun hoe a breating on a blacking the her of brold...  \n",
      "It hang hey bla blabe so blain bla brokets...........B.....\n",
      "\n",
      ", Anyrolle.  \n",
      "But your heart baby baby, broked  \n",
      "I'm so blaw I stoming eye glare.\"  \n",
      "You let me  \n",
      "Blace  \n",
      "I no in the twook frodn't was last  \n",
      "You bring at, baby, hearted me where all the exting man bladies some branked of puck troubly of h \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m target_vector \u001b[38;5;241m=\u001b[39m one_hot_encoder(target_indices)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#train the network and get the final hidden state\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m hprev_val, loss_val, _ \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_gradients\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev_val\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#make predictions on every 500th iteration \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#length of characters we want to predict\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:971\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    968\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    974\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:1214\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1214\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:1394\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1391\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1394\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:1401\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1400\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1403\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:1384\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1382\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/assignments/deeplearning-assignments/venv/lib/python3.9/site-packages/tensorflow/python/client/session.py:1477\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1476\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1477\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0\n",
    "\n",
    "    # select input sentence\n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "\n",
    "    # select output sentence\n",
    "    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "\n",
    "    # get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    # convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    # train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={inputs: input_vector, targets: target_vector, init_state: hprev_val})\n",
    "\n",
    "    # make predictions on every 500th iteration\n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        # length of characters we want to predict\n",
    "        sample_length = 500\n",
    "\n",
    "        # randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "\n",
    "        # sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = data[random_index:random_index + seq_length]\n",
    "\n",
    "        # get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "\n",
    "        # store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "\n",
    "        # for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "\n",
    "        for t in range(sample_length):\n",
    "\n",
    "            # convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "\n",
    "            # compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n",
    "                                                         feed_dict={inputs: sample_input_vector, init_state: sample_prev_state_val})\n",
    "\n",
    "            # we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "\n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "\n",
    "            # store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "\n",
    "        # convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "\n",
    "        # combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "\n",
    "        # predict the predict text on every 50000th iteration\n",
    "        if iteration % 50000 == 0:\n",
    "            print('\\n')\n",
    "            print(' After %d iterations' % (iteration))\n",
    "            print('\\n %s \\n' % (text,))\n",
    "            print('-'*115)\n",
    "\n",
    "    # increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have our output\n",
    "\n",
    "After 850000 iterations\n",
    "\n",
    "Going infong  \n",
    "Traib.  \n",
    "say the right I really baby, I do bla though the up to that you betters.  \n",
    "god over we proming byin'\n",
    "\n",
    ", This somebody broun hoe a breating on a blacking the her of brold...  \n",
    "It hang hey bla blabe so blain bla brokets...........B.....\n",
    "\n",
    ", Anyrolle.  \n",
    "But your heart baby baby, broked  \n",
    "I'm so blaw I stoming eye glare.\"  \n",
    "You let me  \n",
    "Blace  \n",
    "I no in the twook frodn't was last  \n",
    "You bring at, baby, hearted me where all the exting man bladies some branked of puck troubly of h\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
